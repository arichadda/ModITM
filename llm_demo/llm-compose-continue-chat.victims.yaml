version: '3.7'
services:

  codellama:
    container_name: codellama
    hostname: codellama
    restart: always
    build:
      context: codellama
      dockerfile: Dockerfile
      args:
        MODEL_FILE_NAME: ${MODEL_FILE_NAME}
        MODEL_NAME: ${MODEL_NAME}
        MODEL_TEMP: ${MODEL_TEMP}
        MODEL_CONTEXT_LEN: ${MODEL_CONTEXT_LEN}
        MODEL_SYSTEM_PROMPT: ${MODEL_SYSTEM_PROMPT}
        SERVICE_NAME: ${SERVICE_NAME}
    image: 'moditm-llm-codellama:latest'
    expose:
      - 11434
    ports:
      - 11436:11434
    volumes:
      - '${PWD}/ollama:/root/.ollama'
      # - '${PWD}/pcaps:/pcaps'
    # command: serve 
    # healthcheck:
    #   test: curl --fail http://localhost:11434/api/tags || exit 1
    #   interval: 30s
    #   retries: 2
    #   start_period: 20s
    #   timeout: 2s
    networks:
      llm:
        ipv4_address: 192.168.2.221
    env_file:
      - path: .env
        required: true
    tty: true
    
networks:
  llm:
    name: llm
    driver: bridge
    ipam:
      config:
        - subnet: 192.168.2.0/24
          gateway: 192.168.2.1