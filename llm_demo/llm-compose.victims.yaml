version: '3.7'
services:
  nano-bots:
    image: rashleyiqt/nano-bots-api-debug:latest
    build:
      context: nano-bots-api-debug
      dockerfile: Dockerfile
    container_name: nano-bots-api
    environment:
      ENVIRONMENT: development
      PORT: 3048

      RUN_DANGEROUSLY_AS_SUDO: "false"
      RVM: "false"

      FORCE_SANDBOXED: "true"
      ALLOW_CARTRIDGES_PATH_HEADER: "true"

      NANO_BOTS_ENCRYPTION_PASSWORD: UNSAFE
      NANO_BOTS_END_USER: your-user
      NANO_BOTS_CARTRIDGES_PATH: /cartridges
      NANO_BOTS_STATE_PATH: /state
      NANO_BOTS_RACK_ATTACK: "false"
      NANO_BOTS_NEW_RELIC: "false"
      OLLAMA_API_ADDRESS: http://192.168.2.221:11434
    ports:
      - 3048:3048
    volumes:
      - '${HOME}/.local/share/nano-bots/cartridges:/cartridges'
      - '${PWD}/pcaps:/pcaps'
      - '${PWD}/state:/state'
    networks:
      llm:
        ipv4_address: 192.168.2.24
    depends_on:
      model_puller:
        condition: service_completed_successfully

  codellama:
    container_name: codellama
    hostname: codellama
    restart: always
    build:
      context: codellama
      dockerfile: Dockerfile
    image: 'moditm-llm-codellama:latest'
    expose:
      - 11434
    volumes:
      - '${PWD}/ollama:/root/.ollama'
      - '${PWD}/pcaps:/pcaps'
    command: serve 
    healthcheck:
      test: curl --fail http://localhost:11434/api/tags || exit 1
      interval: 30s
      retries: 2
      start_period: 20s
      timeout: 2s
    networks:
      llm:
        ipv4_address: 192.168.2.221
    
  model_puller:
    image: 'curlimages/curl:8.6.0'
    environment:
      OLLAMA_HOST: 'codellama'
      OLLAMA_PORT: 11434
    volumes:
      - '${PWD}/victims/model_puller:/model_puller'
    command: "/model_puller/pull.sh"
    networks:
      - llm
    depends_on:
      codellama:
        condition: service_healthy
networks:
  llm:
    name: llm
    driver: bridge
    ipam:
      config:
        - subnet: 192.168.2.0/24
          gateway: 192.168.2.1